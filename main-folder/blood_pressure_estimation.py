import os
import numpy as np
from pathlib import Path
from scipy import signal
import sys
import joblib

sys.path.append(str(Path(__file__).resolve().parent.parent))
## è„ˆæ³¢å–å¾—ç”¨é–¢æ•°
from blood_pressure_estimation_project.signal_processing.signal import extract_green_signal,bandpass_filter_pulse,detrend_signal
from blood_pressure_estimation_project.signal_processing.peak_detection import detect_pulse_peak
from blood_pressure_estimation_project.signal_processing.visualize import visualize_pulse
## è„ˆæ³¢ç‰¹å¾´é‡å–å¾—ç”¨é–¢æ•°
from blood_pressure_estimation_project.signal_processing.analyze import select_pulses_by_statistics,check_sdppg_waveform,analyze_pulses
from blood_pressure_estimation_project.signal_processing.t_generation import generate_t1,generate_t2
from blood_pressure_estimation_project.signal_processing.get_feature import calc_contour_features,calc_dr_features
from mycommon.read_yaml import load_config
from mycommon.select_folder import select_folder

## æ©Ÿæ¢°å­¦ç¿’ç”¨é–¢æ•°
from blood_pressure_estimation_project.data_loader.load_features import load_features_with_age,load_all_data,df_to_np_arrays
from blood_pressure_estimation_project.ml_models.trainer import estimate_bp
from blood_pressure_estimation_project.save_utils.save_utils import save_predictions_to_txt,save_model,save_metrics_to_txt,save_all_models
from blood_pressure_estimation_project.evaluation.metrics import calc_metrics

def load_model(model_save_dir,model_name, target, select):
    MODEL_SAVE_DIR = model_save_dir
    file_path = os.path.join(MODEL_SAVE_DIR, f"{model_name}_{target}_{select}.pkl")
    if os.path.exists(file_path):
        return joblib.load(file_path)
    else:
        raise FileNotFoundError(f"Model file not found: {file_path}")

def get_pulse_signal(csv_path, save_dir, bandpass_range,sampling_rate, time):
    """1ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾ã—ã¦è„ˆæ³¢å‡¦ç†ãƒ»ä¿å­˜ãƒ»å¯è¦–åŒ–ã‚’å®Ÿè¡Œ"""
    os.makedirs(save_dir, exist_ok=True)

    # Gæˆåˆ†æŠ½å‡º
    pulse_raw = extract_green_signal(csv_path)

    # å‚¾ãé™¤å» & ãƒãƒ³ãƒ‰ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿
    pulse_detrended = detrend_signal(pulse_raw)
    pulse_filtered = bandpass_filter_pulse(pulse_detrended,bandpass_range, sampling_rate)

    # ãƒ”ãƒ¼ã‚¯æ¤œå‡º
    peak_indexes, valley_indexes = detect_pulse_peak(pulse_filtered, sampling_rate)

    # å¯è¦–åŒ– & ä¿å­˜
    save_img_peak = Path(save_dir) / "g_peak.png"

    visualize_pulse(pulse_filtered, save_img_peak, peak_indexes, valley_indexes, sampling_rate, time)

    # ãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã¿è„ˆæ³¢ã‚’ä¿å­˜
    save_path = Path(save_dir) / "g.csv"
    np.savetxt(save_path, pulse_filtered, delimiter=",")


def get_pulse_feature(csv_file,bandpass_range,sampling_rate,resampling_rate):
    
    pulse_bandpass_filtered = np.loadtxt(csv_file, delimiter=",")
    
    # å‚¾ãé™¤å»
    pulse_detrended = signal.detrend(pulse_bandpass_filtered)
    # ãƒãƒ³ãƒ‰ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿å‡¦ç†
    pulse_bandpass_filtered = bandpass_filter_pulse(pulse_detrended,bandpass_range,sampling_rate)
    
    # ãƒ”ãƒ¼ã‚¯æ¤œå‡º (ä¸Šå´ï¼Œä¸‹å´)
    peak_indexes, valley_indexes = detect_pulse_peak(pulse_bandpass_filtered, sampling_rate)
    
    # å„1æ³¢å½¢ã®é¢ç©ï¼ŒæŒç¶šæ™‚é–“ï¼Œæœ€å¤§æŒ¯å¹…ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    area_list = []
    duration_time_list = []
    amplitude_list = []
    acceptable_idx_list = []

    ## è„ˆæ³¢æ³¢å½¢ã®å¾®åˆ†ç­‰ã®è§£æ
    area_list, duration_time_list, amplitude_list, acceptable_idx_list,pulse_waveform_num= analyze_pulses(pulse_bandpass_filtered,valley_indexes)
    acceptable_idx_list =select_pulses_by_statistics(area_list, duration_time_list, amplitude_list, pulse_waveform_num)
    
    ## t1ã‚’æ±‚ã‚ã‚‹
    t1, pulse_waveform_upsampled_list, pulse_waveform_original_list, success = generate_t1(pulse_bandpass_filtered,valley_indexes,amplitude_list,acceptable_idx_list,resampling_rate)
    
    ## t2ã‚’æ±‚ã‚ã‚‹
    t2 =generate_t2(t1, pulse_waveform_upsampled_list, pulse_waveform_original_list, upper_ratio=0.10)
    
    # t2ã‹ã‚‰ç‰¹å¾´é‡ã‚’æ±‚ã‚ã‚‹
    features_cn_array = calc_contour_features(t2,resampling_rate)
    features_dr_array = calc_dr_features(t2,resampling_rate)
    
    return features_cn_array,features_dr_array

def bp_estimation(feature_root,feature_subdir,state_list,feature_cn_num,feature_dr_num,age_dict,bp_root,config_age_path,model_list, use_optuna, n_features_to_select,cv_method,n_splits):
    features_g,subject_names = load_features_with_age(
        base_dir=feature_root,
        feature_dir=feature_subdir,
        state_list=state_list,
        feature_num_cn=feature_cn_num,
        feature_num_dr=feature_dr_num,
        age_dict=age_dict
    )
    ex_num = len(state_list)
    df = load_all_data(feature_root,feature_subdir,bp_root, config_age_path, feature_cn_num, feature_dr_num)
    num_subjects = len(subject_names)
    
    feature_g, bp_array =df_to_np_arrays(df, feature_prefix="feature_", num_trials_per_subject=3)
    # -----------------------------
    # æ¨å®šãƒ»è©•ä¾¡ï¼ˆG, NIR, G+NIR ãã‚Œãã‚Œï¼‰
    # -----------------------------
    
    # print("[INFO] Estimating with RGB-NIR G only")
    mae_list,predictions_dict,selected_features_sbp_count,selected_features_dbp_count,trained_model,selected= estimate_bp(features_g, bp_array, num_subjects,ex_num,
                                        model_list, use_optuna, n_features_to_select,cv_method,n_splits)
    
    # # -----------------------------
    # # æ¨å®šå€¤ä¿å­˜
    # # -----------------------------
    save_predictions_to_txt({
    key: val.tolist() for key, val in predictions_dict.items()})
    
    # # -----------------------------
    # # è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—ãƒ»ä¿å­˜
    # # -----------------------------
    save_metrics_to_txt(mae_list)
    # # -----------------------------
    # # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    # # -----------------------------
    save_all_models(trained_model,selected,"saved_models")
    print("[DONE] All processing completed.")

def bp_predict_with_saved_models(feature_root, feature_subdir, state_list,
                                 feature_cn_num, feature_dr_num, age_dict,
                                 bp_root, config_age_path,
                                 model_list, selection_types, model_dir):
    # ç‰¹å¾´é‡ã¨è¡€åœ§ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    features_g, subject_names = load_features_with_age(
        base_dir=feature_root,
        feature_dir=feature_subdir,
        state_list=state_list,
        feature_num_cn=feature_cn_num,
        feature_num_dr=feature_dr_num,
        age_dict=age_dict
    )
    df = load_all_data(feature_root, feature_subdir, bp_root, config_age_path,
                       feature_cn_num, feature_dr_num)
    
    num_subjects = len(subject_names)
    ex_num = len(state_list)
    features_np, bp_array = df_to_np_arrays(df, feature_prefix="feature_", num_trials_per_subject=ex_num)
    features_np = features_np.reshape(-1, features_np.shape[-1])
    
    age_vector = []
    for subject_name in subject_names:
        age = age_dict.get(subject_name, 0)
        age_vector.extend([age] * ex_num)  # è©¦è¡Œæ•°ã ã‘ç¹°ã‚Šè¿”ã™

    # numpyé…åˆ—ã«ã—ã¦ shape = (num_subjects * ex_num, 1)
    age_vector = np.array(age_vector).reshape(-1, 1)

    # ç‰¹å¾´é‡ã«å¹´é½¢ã‚’è¿½åŠ ï¼ˆshape: (N, D+1)ï¼‰
    features_np = np.hstack([features_np, age_vector])
    sbp_ref, dbp_ref = bp_array[:, 0], bp_array[:, 1]
    predictions_dict = {}
    estimated_arrays = []
    ml_algorithm_feature_list = []

    for model_name in model_list:
        for sel in selection_types:
            key_sbp = f"{model_name}_sbp_{sel}"
            key_dbp = f"{model_name}_dbp_{sel}"
            sbp_model_path = os.path.join(model_dir, f"{key_sbp}.pkl")
            dbp_model_path = os.path.join(model_dir, f"{key_dbp}.pkl")

            if not os.path.exists(sbp_model_path) or not os.path.exists(dbp_model_path):
                print(f"[WARNING] ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {sbp_model_path} ã¾ãŸã¯ {dbp_model_path}")
                continue

            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            sbp_model = joblib.load(sbp_model_path)
            dbp_model = joblib.load(dbp_model_path)
            
            # ğŸ”» RFEãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯é¸æŠã•ã‚ŒãŸç‰¹å¾´é‡ã ã‘æŠ½å‡º
            if sel == "rfe":
                sbp_feat_path = os.path.join(model_dir, f"{model_name}_sbp_rfe_selected_features.npy")
                dbp_feat_path = os.path.join(model_dir, f"{model_name}_dbp_rfe_selected_features.npy")

                if not os.path.exists(sbp_feat_path) or not os.path.exists(dbp_feat_path):
                    print(f"[WARNING] RFEé¸æŠç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {sbp_feat_path}")
                    continue

                sbp_selected_idx = np.load(sbp_feat_path)
                dbp_selected_idx = np.load(dbp_feat_path)

                X_sbp = features_np[:, sbp_selected_idx]
                X_dbp = features_np[:, dbp_selected_idx]
            else:
                X_sbp = features_np
                X_dbp = features_np

            # æ¨å®š
            sbp_pred = sbp_model.predict(X_sbp)
            dbp_pred = dbp_model.predict(X_dbp)

            predictions_dict[key_sbp] = sbp_pred
            predictions_dict[key_dbp] = dbp_pred

            estimated_arrays.append(sbp_pred)
            estimated_arrays.append(dbp_pred)
            ml_algorithm_feature_list.append(f"{model_name}_{sel}")

    # è©•ä¾¡æŒ‡æ¨™è¨ˆç®—
    mae_list = calc_metrics(
    sbp_reference_array=sbp_ref,
    dbp_reference_array=dbp_ref,
    estimated_arrays=estimated_arrays,
    selected_features_sbp=[],  # ç©ºã§OK
    selected_features_dbp=[],  # ç©ºã§OK
    ml_algorithm_feature_list=ml_algorithm_feature_list)

    # çµæœä¿å­˜
    save_predictions_to_txt({k: v.tolist() for k, v in predictions_dict.items()})
    save_metrics_to_txt(mae_list)
    print("[DONE] äºˆæ¸¬ã¨è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    return mae_list, predictions_dict


def main():
    
    #==========config=============================
    current_path = Path(__file__)
    parent_path = current_path.parent     
    config = load_config(str(parent_path)+"\\blood_pressure_config.yaml")
    config_age_path = str(parent_path)+"\\blood_pressure_age.yaml"
    config_age =load_config(config_age_path)

    DEFAULT_BANDPASS_RANGE_HZ = config["preprocessing"]["bandpass_range_hz"]
    DEFAULT_SAMPLING_RATE     = config["preprocessing"]["sampling_rate"]
    DEFAULT_CAPTURE_TIME      = config["preprocessing"]["capture_time"]
    DEFAULT_OUTPUT_FOLDER_NAME = config["preprocessing"]["output_folder_name"]
    DEFAULT_RESAMPLING_RATE  =config["feature_extraction"]["resampling_rate"]

    feature_root = config["feature_root"]
    bp_root = config["bp_folder_root"]
    feature_subdir = config["feature_subdir"]
    feature_cn_num = config["feature_cn_num"]
    feature_dr_num = config["feature_dr_num"]
    model_list = config["models_to_use"]
    use_optuna = config["use_optuna"]
    n_features_to_select = config["n_features_to_select"]
    state_list = config["state_list"]
    ex_num  = len(state_list)
    age_dict = config_age.get("subject_ages", {})  
    cv_method = config["cv_method"]
    n_splits = config["n_splits"]
    model_dir = config["model_dir"]
    selection_types = config["selection_types"]
    #=======================================
    
    
    ##ç”»åƒã‹ã‚‰å–å¾—ã—ãŸ.csvãƒ•ã‚¡ã‚¤ãƒ«ãŒå…¥ã£ãŸè¦ªãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠã™ã‚‹
    # INPUT_PULSE_DATA = select_folder("è„ˆæ³¢ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™")
    # INPUT_BP_DATA = select_folder("è¡€åœ§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã™")
    
    INPUT_PULSE_DATA =feature_root
    ## ===========è„ˆæ³¢å–å¾—å‡¦ç† ==================================
    input_path = Path(INPUT_PULSE_DATA)
    csv_files = []
    for subject_folder in input_path.iterdir():
        if subject_folder.is_dir():
            # å„ã‚µãƒ–ãƒ•ã‚©ãƒ«ãƒ€ã®ä¸­ã®csvãƒ•ã‚¡ã‚¤ãƒ«ã ã‘æ¢ã™ï¼ˆå†å¸°ã—ãªã„ï¼‰
            csv_in_subfolder = list(subject_folder.glob("*.csv"))
            csv_files.extend(csv_in_subfolder)
    
    for csv_path in csv_files:
        csv_stem = csv_path.stem
        save_dir = Path(csv_path).parent / csv_stem / "processed"
        get_pulse_signal(csv_path, save_dir,DEFAULT_BANDPASS_RANGE_HZ, DEFAULT_SAMPLING_RATE,DEFAULT_CAPTURE_TIME)
        print(f"{csv_path} is Done !!")
        
        
    ## ===========è„ˆæ³¢ç‰¹å¾´é‡å–å¾—å‡¦ç† ==================================
    processed_csv_files = []

    for subject_folder in input_path.iterdir():
        if subject_folder.is_dir():
            for ex_folder in subject_folder.iterdir():
                if ex_folder.is_dir():    
                # å„è¢«é¨“è€…ãƒ•ã‚©ãƒ«ãƒ€
                    processed_folder = ex_folder / "processed"
                    if processed_folder.exists():
                        csvs = list(processed_folder.glob("*.csv"))
                        processed_csv_files.extend(csvs)

    for csv_path in processed_csv_files:
        features_cn_array,features_dr_array =get_pulse_feature(csv_path,DEFAULT_BANDPASS_RANGE_HZ,DEFAULT_SAMPLING_RATE,DEFAULT_RESAMPLING_RATE)
        csv_stem = csv_path.stem  # ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
        dir_path = Path(csv_path).parents[1]
        print(dir_path)
        # ç‰¹å¾´é‡ä¿å­˜ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã®æŒ‡å®š
        dir_name_save = str(dir_path)  + "\\"+ "features\\"
        os.makedirs(dir_name_save, exist_ok=True)

        # ç‰¹å¾´é‡ä¿å­˜ç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«åã®æŒ‡å®š
        filename_save_features_cn = dir_name_save + "features_cn" + ".csv"
        filename_save_features_dr = dir_name_save + "features_dr" + ".csv"

        # ç‰¹å¾´é‡ã®ä¿å­˜
        np.savetxt(filename_save_features_cn, features_cn_array, delimiter=",")
        np.savetxt(filename_save_features_dr, features_dr_array, delimiter=",")
        print(csv_path, "is Done!")
        
    ## ===========æ©Ÿæ¢°å­¦ç¿’ ==================================
    bp_estimation(feature_root,feature_subdir,state_list,feature_cn_num,feature_dr_num,age_dict,bp_root,config_age_path,model_list, use_optuna, n_features_to_select,cv_method,n_splits)
    # bp_predict_with_saved_models(feature_root, feature_subdir, state_list,feature_cn_num, feature_dr_num, age_dict,bp_root, config_age_path,model_list, selection_types, model_dir)


if __name__ == "__main__":
    main()